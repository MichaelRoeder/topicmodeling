/**
 * This file is part of topicmodeling.io.
 *
 * topicmodeling.io is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Lesser General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * topicmodeling.io is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU Lesser General Public License for more details.
 *
 * You should have received a copy of the GNU Lesser General Public License
 * along with topicmodeling.io.  If not, see <http://www.gnu.org/licenses/>.
 */
package org.dice_research.topicmodeling.io;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.Reader;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.IntStream;

import org.apache.commons.lang3.NotImplementedException;
import org.dice_research.topicmodeling.algorithms.Model;
import org.dice_research.topicmodeling.algorithms.LDAModel;
import org.dice_research.topicmodeling.algorithms.ModelingAlgorithm;
import org.dice_research.topicmodeling.algorithms.ProbTopicModelingAlgorithmStateSupplier;
import org.dice_research.topicmodeling.algorithms.WordCounter;
import org.dice_research.topicmodeling.io.xml.XMLParserObserver;
import org.dice_research.topicmodeling.io.xml.stream.SimpleReaderBasedXMLParser;
import org.dice_research.topicmodeling.utils.doc.Document;
import org.dice_research.topicmodeling.utils.doc.DocumentClassificationResult;
import org.dice_research.topicmodeling.utils.doc.DocumentWordCounts;
import org.dice_research.topicmodeling.utils.vocabulary.SimpleVocabularyBuilder;
import org.dice_research.topicmodeling.utils.vocabulary.Vocabulary;
import org.dice_research.topicmodeling.utils.vocabulary.VocabularyMapping;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Reads XML LDA model state files generated by LODCat (lodcat.gensim/ldamodel2xml).
 */
public class LodcatProbTopicModelingAlgorithmStateReader extends ProbTopicModelingAlgorithmStateReader {
    private static final Logger logger = LoggerFactory.getLogger(LodcatProbTopicModelingAlgorithmStateReader.class);

    /**
     * Reads XML LDA model state file generated by LODCat (lodcat.gensim/ldamodel2xml).
     *
     * @return State supplier for the file data.
     */
    @Override
    public ProbTopicModelingAlgorithmStateSupplier readProbTopicModelState(File file) {
        ProbTopicModelingAlgorithmStateSupplier result = null;
        try (InputStream in = new FileInputStream(file)) {
            result = readProbTopicModelState(in);
        } catch (FileNotFoundException e) {
            logger.error("Error while trying to read serialized ProbTopicModelingAlgorithmStateSupplier from file", e);
        } catch (IOException e) {
            logger.error("Error while trying to read serialized ProbTopicModelingAlgorithmStateSupplier from file", e);
        }
        return result;
    }

    /**
     * Reads XML LDA model state stream generated by LODCat (lodcat.gensim/ldamodel2xml).
     *
     * @return State supplier for the stream data.
     */
    @Override
    public ProbTopicModelingAlgorithmStateSupplier readProbTopicModelState(InputStream in) throws IOException {
        Reader reader = new InputStreamReader(in);
        LodcatProbTopicModelingAlgorithmStateSupplier observer = new LodcatProbTopicModelingAlgorithmStateSupplier();
        new SimpleReaderBasedXMLParser(reader, observer).parse();
        return observer;
    }

    public class LodcatProbTopicModelingAlgorithmStateSupplier
        implements XMLParserObserver, ProbTopicModelingAlgorithmStateSupplier, ModelingAlgorithm {
        private double beta = 0;
        private Vocabulary vocab = null;
        private SimpleVocabularyBuilder vocabBuilder = new SimpleVocabularyBuilder();
        private int numberOfTopics = 0;
        private int numberOfDocuments = 0;
        private int numberOfWords = 0;
        private String data = "";
        private Map<String, String> attributes;
        private IntStream.Builder documentWordsBuilder;
        private IntStream.Builder documentWordTopicsBuilder;
        //private IntStream.Builder documentWordFreqsBuilder;
        private List<int[]> documentWords = new ArrayList<>();
        private List<int[]> documentWordTopics = new ArrayList<>();
        //private List<int[]> documentWordFreqs = new ArrayList<>();

        public double getBeta() {
            return beta;
        }

        @Override
        public Vocabulary getVocabulary() {
            return vocab;
        }

        @Override
        public int[] getWordTopicAssignmentForDocument(int documentId) {
            return documentWordTopics.get(documentId);
        }

        @Override
        public int[] getWordsOfDocument(int documentId) {
            return documentWords.get(documentId);
        }

        @Override
        public WordCounter getWordCounts() {
            throw new NotImplementedException();
        }

        @Override
        public int getNumberOfTopics() {
            return numberOfTopics;
        }

        @Override
        public int getNumberOfDocuments() {
            return numberOfDocuments;
        }

        @Override
        public int getNumberOfWords() {
            return vocab != null ? vocab.size() : 0;
        }

        @Override
        public long getSeed() {
            throw new NotImplementedException();
        }

        @Override
        public void handleOpeningTag(String tagString) {
            // this is not perfect
            String[] items = tagString.split("\\s+");
            if (items.length > 1) {
                attributes = new HashMap<>();
                for (int i = 1; i < items.length; i++) {
                    if (items[i].indexOf("=") != -1) {
                        String[] kv = items[i].split("=", 2);
                        attributes.put(kv[0], kv[1].substring(1, kv[1].length() - 1));
                    }
                }
            }
            switch (items[0]) {
            case "document":
                documentWordsBuilder = IntStream.builder();
                documentWordTopicsBuilder = IntStream.builder();
                //documentWordFreqsBuilder = IntStream.builder();
                break;
            case "t": // token
                int freq = Integer.parseInt(attributes.get("freq"));
                int id = Integer.parseInt(attributes.get("id"));
                int topic = Integer.parseInt(attributes.get("topic"));
                if (topic != -1 ) {
                    for (int i = 0; i < freq; i++) {
                        documentWordsBuilder.accept(id);
                        documentWordTopicsBuilder.accept(topic);
                    }
                    //documentWordFreqsBuilder.accept(Integer.parseInt(attributes.get("freq")));
                }
            }
        }

        @Override
        public void handleClosingTag(String tagString) {
            switch (tagString) {
            case "eta":
                beta = Double.parseDouble(data);
                break;
            case "word":
                vocabBuilder.setWord(data, Integer.parseInt(attributes.get("id")));
                break;
            case "dictionary":
                vocab = vocabBuilder.getVocabulary();
                break;
            case "numberOfTopics":
                numberOfTopics = Integer.parseInt(data);
                break;
            case "numberOfWords":
                numberOfWords = Integer.parseInt(data);
                break;
            case "document":
                documentWords.add(documentWordsBuilder.build().toArray());
                documentWordTopics.add(documentWordTopicsBuilder.build().toArray());
                //documentWordFreqs.add(documentWordFreqsBuilder.build().toArray());
                documentWordsBuilder = null;
                documentWordTopicsBuilder = null;
                //documentWordFreqsBuilder = null;
                break;
            case "documents":
                numberOfDocuments = documentWords.size();
            }
            data = "";
            attributes = null;
        }

        @Override
        public void handleEmptyTag(String tagString) {
            handleOpeningTag(tagString);
        }

        @Override
        public void handleData(String data) {
            this.data = data;
        }

        @Override
        public void initialize(Corpus corpus) {
            throw new NotImplementedException();
        }

        public void performNextStep() {
            throw new NotImplementedException();
        }

        public Model getModel() {
            // not needed for ArunModelSelectionEvaluator
            return null;
        }

    }
}
